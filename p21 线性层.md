#### 介绍

过了下pytorch中的各种层吧，只能说，学会自己看官方文档。

```python
import torch
import torchvision
from torch import nn
from torch.nn import Linear
from torch.utils.data import DataLoader
```

```python
dataset = torchvision.datasets.CIFAR10("../data", train=False, transform=torchvision.transforms.ToTensor(),
                                       download=True)

dataloader = DataLoader(dataset, batch_size=64,drop_last=True)


class Tudui(nn.Module):
    def __init__(self):
        super(Tudui, self).__init__()
        self.linear1 = Linear(196608, 10)

    def forward(self, input):
        output = self.linear1(input)
        return output


tudui = Tudui()

for data in dataloader:
    imgs, targets = data
    # print(imgs.shape)
    # output = torch.reshape(imgs, (1, 1, 1, -1))
    output = torch.flatten(imgs)  # 展平(1行)
    print(output.shape)
    output = tudui(output)
    print(output.shape)

'''
正则化层  Normalization Layers   nn.BatchNorm2d 
有一篇论文，意思是正则化层可以提高训练速度

参数只有一个，channel中的C，num_feature, 令其跟 channel 数相同即可，官方文档有个事例：

>>> # With Learnable Parameters
>>> m = nn.BatchNorm2d(100)
>>> # Without Learnable Parameters           # 不含可学习参数
>>> m = nn.BatchNorm2d(100, affine=False)    # 这里的 100，是跟着下一行的100（channel）设置的
>>> input = torch.randn(20, 100, 35, 45)
>>> output = m(input)

'''

'''
官方文档有一些写好的网络
'''

```

这块太熟悉了，不展开了。